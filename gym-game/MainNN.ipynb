{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/esha/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Maze/Maze.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Maths/Cord.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Maze/MazeGenerator.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Agents/Worker.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Maths/Action.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Maths/State.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Agents/Prey.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Agents/Agent.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Main/Simulator.ipynb\n",
      "Importing Jupyter notebook from /home/esha/anaconda3/Scripts/gym-game/Windows/MainWindow.ipynb\n",
      "[0, 1, 2, 3, 4]\n",
      "Name: Test\n",
      "6x6\n",
      "Start: (4, 5)\n",
      "End: (1, 0) \n",
      "111121 5\n",
      "110001 4\n",
      "100011 3\n",
      "101001 2\n",
      "100011 1\n",
      "131111 0\n",
      "Test Run: 1, No of Steps: 72257 Total Reward: -2247256 9 71298.0\n",
      "Test Run: 2, No of Steps: 9429 Total Reward: -146498 9 9276.0\n",
      "Test Run: 3, No of Steps: 4548 Total Reward: -278758 9 4334.0\n",
      "Test Run: 4, No of Steps: 25490 Total Reward: -5722632 9 20143.0\n",
      "Test Run: 5, No of Steps: 30761 Total Reward: -1623597 9 29760.0\n",
      "Test Run: 6, No of Steps: 11482 Total Reward: -323412 9 11376.0\n",
      "Test Run: 7, No of Steps: 6888 Total Reward: -257375 9 6734.0\n",
      "Test Run: 8, No of Steps: 7216 Total Reward: -368818 9 6955.0\n",
      "Test Run: 9, No of Steps: 7348 Total Reward: -394798 9 7066.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f929a4634918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f929a4634918>\u001b[0m in \u001b[0;36mGame\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m#print(\"Action \",action, env.action_space[action])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mstate_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;31m#reward = reward if not terminal else -reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mstate_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/Scripts/gym-game/gym_game/envs/game_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m#print(p.getName(), oldPosition.CordToString(), \" to \",p.getPos().CordToString(),Action(self.action_space[action][index]))#, \"Moving\",self.maze.returnMoving())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mstate_Next\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mreward\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moldPosition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mwallMove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/Scripts/gym-game/Agents/Worker.ipynb\u001b[0m in \u001b[0;36mgetView\u001b[0;34m(self, c, span)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m\"                if(view not in views):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"                    views.append(view)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;34m\"        return views\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"    \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;34m\"    def getView(self,c, span):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym_game\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#from scores.score_logger import ScoreLogger\n",
    "\n",
    "ENV_NAME = \"game-v0\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "\n",
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        #print(\"Q_values \",q_values[0],\"Max \",np.argmax(q_values[0]))\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "def Game():\n",
    "    env = gym.make(ENV_NAME)                                     #\n",
    "    #score_logger = ScoreLogger(ENV_NAME)\n",
    "    observation_space = int(env.observation_space )         #\n",
    "    action_space = len(env.action_space)                            #\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    run = 0\n",
    "    count=0\n",
    "    while True:\n",
    "        run += 1\n",
    "        #state=env.reset()\n",
    "        if(count<5):\n",
    "            state = env.reset()  \n",
    "        else:\n",
    "            print(\" \")\n",
    "            state=env.resetNewMaze()\n",
    "            count=0\n",
    "            \n",
    "        state = np.reshape(state,  [1,observation_space])\n",
    "        step = 0\n",
    "        stepsNo=0\n",
    "        totReward=0\n",
    "        while True:\n",
    "            step += 1\n",
    "            #env.render()\n",
    "            action = dqn_solver.act(state)\n",
    "            #print(\"Action \",action, env.action_space[action])\n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            #reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1,observation_space])\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            stepsNo+=1\n",
    "            totReward+=reward\n",
    "            if (terminal):\n",
    "                print (env.maze.name,\"Run: \" + str(run) + \", No of Steps: \" + str(step), \"Total Reward:\",totReward,env.shortestRoute, env.count/env.number)\n",
    "                file=open(\"RunsData.txt\",\"a+\")\n",
    "                file.write(env.maze.name+\" \"+str(run)+\" \"+str(step)+\" \"+str(totReward)+\" \"+str(env.count/env.number)+\"\\n\")\n",
    "                file.close()\n",
    "                if(step<=env.shortestRoute):\n",
    "                    count+=0\n",
    "                break\n",
    "            dqn_solver.experience_replay()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Game()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([[4],[5],[6]])\n",
    "print(a)\n",
    "a=np.append(a,[[7]], axis=0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.empty([1,1])\n",
    "b=np.append(b,[[1]],axis=0)\n",
    "b=np.append(b,[[2]],axis=0)\n",
    "b=np.delete(b,0,axis=0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Maths.Action import Action\n",
    "action_space=[]\n",
    "number=2        \n",
    "for i in range(0,len(Action)):\n",
    "    action_space.append(Action(i))\n",
    "import itertools\n",
    "possible_actions=[action_space]*number\n",
    "print(action_space)\n",
    "self_action_space=np.asarray(list(itertools.product(*possible_actions)))\n",
    "print(self_action_space[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
